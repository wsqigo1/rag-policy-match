import re
import fitz  # PyMuPDF
import pdfplumber
import docx2txt
import jieba
import hashlib
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import logging
# from docx import Document as DocxDocument  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œéœ€è¦æ—¶å†å¯ç”¨

from models import PolicyDocument, PolicyChunk, StructuredPolicy
from config import config, Config

logger = logging.getLogger(__name__)

class StructuredPolicyExtractor:
    """ç»“æ„åŒ–æ”¿ç­–ä¿¡æ¯æå–å™¨"""
    
    def __init__(self):
        # ç»“æ„åŒ–å­—æ®µçš„æ­£åˆ™æ¨¡å¼
        self.field_patterns = {
            'basis_document': [
                r'ä¾æ®æ–‡ä»¶[ï¼š:]\s*(.*?)(?=\n|$)',
                r'åˆ¶å®šä¾æ®[ï¼š:]\s*(.*?)(?=\n|$)',
                r'æ”¿ç­–ä¾æ®[ï¼š:]\s*(.*?)(?=\n|$)'
            ],
            'issuing_agency': [
                r'å‘æ–‡æœºæ„[ï¼š:]\s*(.*?)(?=\n|$)',
                r'åˆ¶å®šå•ä½[ï¼š:]\s*(.*?)(?=\n|$)',
                r'å‘å¸ƒå•ä½[ï¼š:]\s*(.*?)(?=\n|$)',
                r'ä¸»ç®¡éƒ¨é—¨[ï¼š:]\s*(.*?)(?=\n|$)'
            ],
            'document_number': [
                r'å‘æ–‡å­—å·[ï¼š:]\s*(.*?)(?=\n|$)',
                r'æ–‡ä»¶ç¼–å·[ï¼š:]\s*(.*?)(?=\n|$)',
                r'([äº¬æ´¥æ²ªæ¸é»‘å‰è¾½è’™å†€è±«é²è‹çš–æµ™é—½èµ£æ¹˜é„‚æ¡‚ç¼å·è´µæ»‡è—é™•ç”˜å®é’æ–°].*?[ã€”\[]\d{4}[ã€•\]]\s*\d+\s*å·)',
            ],
            'issue_date': [
                r'å‘å¸ƒæ—¥æœŸ[ï¼š:]\s*(.*?)(?=\n|$)',
                r'å°å‘æ—¥æœŸ[ï¼š:]\s*(.*?)(?=\n|$)',
                r'ç”Ÿæ•ˆæ—¥æœŸ[ï¼š:]\s*(.*?)(?=\n|$)',
                r'(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}[æ—¥]?)',
            ],
            'tool_category': [
                r'å·¥å…·åˆ†ç±»[ï¼š:]\s*(.*?)(?=\n|$)',
                r'æ”¿ç­–ç±»å‹[ï¼š:]\s*(.*?)(?=\n|$)',
                r'æ”¯æŒæ–¹å¼[ï¼š:]\s*(.*?)(?=\n|$)',
                r'(èµ„é‡‘æ”¯æŒ|æ”¿ç­–æ”¯æŒ|ç¨æ”¶ä¼˜æƒ |å¹³å°æ”¯æŒ|äººæ‰æ”¯æŒ)'
            ],
            'service_object': [
                r'æœåŠ¡å¯¹è±¡[ï¼š:]\s*(.*?)(?=\n|$)',
                r'é€‚ç”¨å¯¹è±¡[ï¼š:]\s*(.*?)(?=\n|$)',
                r'ç”³è¯·ä¸»ä½“[ï¼š:]\s*(.*?)(?=\n|$)'
            ],
            'service_content': [
                r'æœåŠ¡å†…å®¹[ï¼š:]\s*(.*?)(?=\n|$)',
                r'æ”¯æŒå†…å®¹[ï¼š:]\s*(.*?)(?=\n|$)',
                r'ä¸»è¦å†…å®¹[ï¼š:]\s*(.*?)(?=\n|$)'
            ],
            'condition_requirements': [
                r'æ¡ä»¶è¦æ±‚[ï¼š:]\s*(.*?)(?=\n|$)',
                r'ç”³è¯·æ¡ä»¶[ï¼š:]\s*(.*?)(?=\n|$)',
                r'åŸºæœ¬æ¡ä»¶[ï¼š:]\s*(.*?)(?=\n|$)',
                r'èµ„æ ¼è¦æ±‚[ï¼š:]\s*(.*?)(?=\n|$)'
            ],
            'service_process': [
                r'æœåŠ¡æµç¨‹[ï¼š:]\s*(.*?)(?=\n|$)',
                r'ç”³è¯·æµç¨‹[ï¼š:]\s*(.*?)(?=\n|$)',
                r'åŠç†æµç¨‹[ï¼š:]\s*(.*?)(?=\n|$)'
            ],
            'time_frequency': [
                r'æ—¶é—´[/ï¼]é¢‘åº¦[ï¼š:]\s*(.*?)(?=\n|$)',
                r'ç”³è¯·æ—¶é—´[ï¼š:]\s*(.*?)(?=\n|$)',
                r'å—ç†æ—¶é—´[ï¼š:]\s*(.*?)(?=\n|$)',
                r'å¸¸å¹´å—ç†|éšæ—¶ç”³è¯·|æŒ‰æ‰¹æ¬¡|æ¯å¹´\d+æ¬¡|å®šæœŸ'
            ],
            'contact_info': [
                r'è”ç»œæ–¹å¼[ï¼š:]\s*(.*?)(?=\n|$)',
                r'è”ç³»æ–¹å¼[ï¼š:]\s*(.*?)(?=\n|$)',
                r'å’¨è¯¢ç”µè¯[ï¼š:]\s*(.*?)(?=\n|$)',
                r'(\d{3,4}[-\s]?\d{7,8}|\d{11})'
            ]
        }
        
        # å…³é”®ä¿¡æ¯æå–æ¨¡å¼
        self.analysis_patterns = {
            'industries': [
                r'(ç”Ÿç‰©åŒ»è¯|äººå·¥æ™ºèƒ½|é›†æˆç”µè·¯|æ–°èƒ½æº|æ–°ææ–™|é«˜ç«¯è£…å¤‡|èŠ‚èƒ½ç¯ä¿|æ•°å­—åˆ›æ„|è½¯ä»¶ä¿¡æ¯|èˆªç©ºèˆªå¤©|æµ·æ´‹å·¥ç¨‹)',
                r'(åˆ¶é€ ä¸š|æœåŠ¡ä¸š|å†œä¸š|å»ºç­‘ä¸š|é‡‡çŸ¿ä¸š|é‡‘èä¸š|æˆ¿åœ°äº§ä¸š|æ‰¹å‘é›¶å”®ä¸š|äº¤é€šè¿è¾“ä¸š|ä¿¡æ¯æŠ€æœ¯ä¸š)'
            ],
            'enterprise_scales': [
                r'(å¤§å‹ä¼ä¸š|ä¸­å‹ä¼ä¸š|å°å‹ä¼ä¸š|å¾®å‹ä¼ä¸š)',
                r'(åˆåˆ›ä¼ä¸š|æˆé•¿å‹ä¼ä¸š|æˆç†Ÿä¼ä¸š)',
                r'(ä¸Šå¸‚å…¬å¸|éä¸Šå¸‚å…¬å¸)',
                r'(å›½æœ‰ä¼ä¸š|æ°‘è¥ä¼ä¸š|å¤–èµ„ä¼ä¸š)'
            ],
            'support_amounts': [
                r'(\d+(?:\.\d+)?)\s*(?:ä¸‡å…ƒ|ä¸‡|ä¸‡äººæ°‘å¸)',
                r'(\d+(?:\.\d+)?)\s*(?:äº¿å…ƒ|äº¿)',
                r'æœ€é«˜[æ”¯æŒ]?(?:é‡‘é¢)?[ï¼š:]?\s*(\d+(?:\.\d+)?)\s*(?:ä¸‡å…ƒ|ä¸‡|äº¿å…ƒ|äº¿)',
                r'ä¸è¶…è¿‡\s*(\d+(?:\.\d+)?)\s*(?:ä¸‡å…ƒ|ä¸‡|äº¿å…ƒ|äº¿)'
            ]
        }

    def extract_structured_fields(self, content: str) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–å­—æ®µ"""
        extracted = {}
        
        # æå–ä¸»è¦ç»“æ„åŒ–å­—æ®µ
        for field_name, patterns in self.field_patterns.items():
            extracted[field_name] = self._extract_field_value(content, patterns)
        
        # æå–åˆ†æå­—æ®µ
        extracted['industries'] = self._extract_list_values(content, self.analysis_patterns['industries'])
        extracted['enterprise_scales'] = self._extract_list_values(content, self.analysis_patterns['enterprise_scales'])
        extracted['support_amount_range'] = self._extract_support_amounts(content)
        
        return extracted

    def _extract_field_value(self, content: str, patterns: List[str]) -> Optional[str]:
        """æå–å•ä¸ªå­—æ®µå€¼"""
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                if match.groups():
                    value = match.group(1).strip()
                    if value and len(value) > 3:  # è¿‡æ»¤å¤ªçŸ­çš„åŒ¹é…
                        return value
                else:
                    return match.group(0).strip()
        return None

    def _extract_list_values(self, content: str, patterns: List[str]) -> List[str]:
        """æå–åˆ—è¡¨å€¼"""
        values = set()
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                if match.groups():
                    values.add(match.group(1))
                else:
                    values.add(match.group(0))
        return list(values)

    def _extract_support_amounts(self, content: str) -> Dict[str, Any]:
        """æå–æ”¯æŒé‡‘é¢èŒƒå›´"""
        amounts = []
        for pattern in self.analysis_patterns['support_amounts']:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                try:
                    amount = float(match.group(1))
                    if 'äº¿' in match.group(0):
                        amount *= 10000  # è½¬æ¢ä¸ºä¸‡å…ƒ
                    amounts.append(amount)
                except (ValueError, IndexError):
                    continue
        
        if amounts:
            return {
                'min_amount': min(amounts),
                'max_amount': max(amounts),
                'amounts': amounts
            }
        return {}

class DocumentProcessor:
    """å¢å¼ºçš„æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.extractor = StructuredPolicyExtractor()
        
        # åˆå§‹åŒ–æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.money_pattern = re.compile(r'(\d+(?:\.\d+)?(?:ä¸‡|åƒ|äº¿)?å…ƒ?)')
        self.requirement_pattern = re.compile(r'(?:åº”å½“|å¿…é¡»|éœ€è¦|è¦æ±‚|æ¡ä»¶|èµ„æ ¼|æ ‡å‡†)', re.IGNORECASE)
        self.title_pattern = re.compile(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€ï¼.]', re.IGNORECASE)
        self.section_pattern = re.compile(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡æ¬¾]', re.IGNORECASE)
        
    def process_pdf(self, pdf_path: str) -> PolicyDocument:
        """å¤„ç†PDFæ–‡æ¡£å¹¶æå–ç»“æ„åŒ–ä¿¡æ¯"""
        logger.info(f"Processing PDF: {pdf_path}")
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                full_text = ""
                pages_content = []
                
                for i, page in enumerate(pdf.pages):
                    page_text = page.extract_text() or ""
                    full_text += page_text + "\n"
                    pages_content.append({
                        'page_num': i + 1,
                        'content': page_text
                    })
                
                # æå–ç»“æ„åŒ–å­—æ®µ
                structured_fields = self.extractor.extract_structured_fields(full_text)
                
                # åˆ›å»ºç»“æ„åŒ–æ”¿ç­–å¯¹è±¡
                structured_policy = StructuredPolicy(
                    policy_id=Path(pdf_path).stem,
                    title=self._extract_title(full_text),
                    full_content=full_text,
                    original_filename=Path(pdf_path).name,  # æ·»åŠ åŸå§‹æ–‡ä»¶å
                    file_path=pdf_path,  # æ·»åŠ æ–‡ä»¶è·¯å¾„
                    **structured_fields
                )
                
                # åˆ›å»ºåˆ†å—
                chunks = self._create_enhanced_chunks(
                    full_text, 
                    pages_content, 
                    structured_policy
                )
                structured_policy.chunks = chunks
                
                return PolicyDocument(
                    document_id=Path(pdf_path).stem,
                    title=structured_policy.title,
                    content=full_text,
                    chunks=chunks,
                    metadata={
                        'source': pdf_path,
                        'type': 'pdf',
                        'structured_policy': structured_policy.__dict__
                    }
                )
                
        except Exception as e:
            logger.error(f"Error processing PDF {pdf_path}: {str(e)}")
            raise

    def _extract_title(self, content: str) -> str:
        """æå–æ”¿ç­–æ ‡é¢˜"""
        lines = content.split('\n')
        for line in lines[:10]:  # æ£€æŸ¥å‰10è¡Œ
            line = line.strip()
            if len(line) > 10 and not line.startswith(('é™„ä»¶', 'æ–‡ä»¶', 'ç¼–å·')):
                return line
        return "æœªçŸ¥æ”¿ç­–æ ‡é¢˜"

    def _create_enhanced_chunks(self, content: str, pages_content: List[Dict], 
                               structured_policy: StructuredPolicy) -> List[PolicyChunk]:
        """åˆ›å»ºå¢å¼ºçš„åˆ†å—ï¼ŒåŒ…å«ç»“æ„åŒ–ä¿¡æ¯"""
        chunks = []
        
        # åˆ†æ®µå¤„ç†
        sections = self._split_into_sections(content)
        
        for i, section in enumerate(sections):
            if len(section.strip()) < 50:  # è·³è¿‡å¤ªçŸ­çš„æ®µè½
                continue
                
            chunk_id = f"{structured_policy.policy_id}_chunk_{i}"
            
            # ç¡®å®šchunkç±»å‹å’Œé‡è¦æ€§
            chunk_type, section_name = self._classify_chunk(section)
            
            chunk = PolicyChunk(
                chunk_id=chunk_id,
                policy_id=structured_policy.policy_id,
                content=section,
                section=section_name,
                chunk_type=chunk_type,
                keywords=self._extract_keywords(section),
                
                # ç»§æ‰¿ç»“æ„åŒ–å­—æ®µ
                basis_document=structured_policy.basis_document,
                issuing_agency=structured_policy.issuing_agency,
                document_number=structured_policy.document_number,
                issue_date=structured_policy.issue_date,
                tool_category=structured_policy.tool_category,
                service_object=structured_policy.service_object,
                service_content=structured_policy.service_content,
                condition_requirements=structured_policy.condition_requirements,
                service_process=structured_policy.service_process,
                time_frequency=structured_policy.time_frequency,
                contact_info=structured_policy.contact_info,
                
                # åˆ†æç›¸å…³å­—æ®µ
                policy_level=self._determine_policy_level(structured_policy.issuing_agency),
                support_amount=str(structured_policy.support_amount_range) if structured_policy.support_amount_range else None,
            )
            
            chunks.append(chunk)
        
        return chunks

    def _split_into_sections(self, content: str) -> List[str]:
        """æ™ºèƒ½åˆ†æ®µ"""
        # æŒ‰æ ‡é¢˜å’Œæ®µè½åˆ†å‰²
        patterns = [
            r'\n\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ã€ï¼.]\s*',  # ä¸­æ–‡æ•°å­—æ ‡é¢˜
            r'\n\s*\d+\s*[ã€ï¼.]\s*',  # é˜¿æ‹‰ä¼¯æ•°å­—æ ‡é¢˜
            r'\n\s*[ï¼ˆ(]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ï¼‰)]\s*',  # æ‹¬å·ä¸­æ–‡æ•°å­—
            r'\n\s*[ï¼ˆ(]\s*\d+\s*[ï¼‰)]\s*',  # æ‹¬å·é˜¿æ‹‰ä¼¯æ•°å­—
            r'\n\s*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡æ¬¾é¡¹]\s*',  # ç¬¬Xç« èŠ‚
        ]
        
        # æ‰¾åˆ°æ‰€æœ‰åˆ†å‰²ç‚¹
        split_points = [0]
        for pattern in patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                split_points.append(match.start())
        
        split_points.append(len(content))
        split_points = sorted(set(split_points))
        
        # åˆ›å»ºæ®µè½
        sections = []
        for i in range(len(split_points) - 1):
            section = content[split_points[i]:split_points[i + 1]].strip()
            if section:
                sections.append(section)
        
        return sections

    def _classify_chunk(self, content: str) -> Tuple[str, str]:
        """åˆ†ç±»chunkç±»å‹å’Œæ®µè½åç§°"""
        content_lower = content.lower()
        
        # æ ¹æ®å†…å®¹åˆ¤æ–­ç±»å‹å’Œåç§°
        if any(keyword in content_lower for keyword in ['æ¡ä»¶è¦æ±‚', 'ç”³è¯·æ¡ä»¶', 'åŸºæœ¬æ¡ä»¶']):
            return 'condition', 'æ¡ä»¶è¦æ±‚'
        elif any(keyword in content_lower for keyword in ['æœåŠ¡å†…å®¹', 'æ”¯æŒå†…å®¹', 'ä¸»è¦å†…å®¹']):
            return 'service', 'æœåŠ¡å†…å®¹'
        elif any(keyword in content_lower for keyword in ['æœåŠ¡æµç¨‹', 'ç”³è¯·æµç¨‹', 'åŠç†æµç¨‹']):
            return 'process', 'æœåŠ¡æµç¨‹'
        elif any(keyword in content_lower for keyword in ['è”ç³»æ–¹å¼', 'è”ç»œæ–¹å¼', 'å’¨è¯¢ç”µè¯']):
            return 'contact', 'è”ç»œæ–¹å¼'
        elif any(keyword in content_lower for keyword in ['æœåŠ¡å¯¹è±¡', 'é€‚ç”¨å¯¹è±¡']):
            return 'target', 'æœåŠ¡å¯¹è±¡'
        else:
            return 'general', 'ä¸€èˆ¬å†…å®¹'

    def _extract_keywords(self, content: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        import jieba
        
        # åˆ†è¯
        words = jieba.lcut(content)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ç­‰', 'åŠ', 'å¯¹', 'ä¸º', 'ä»¥', 'æŒ‰', 'ç”±'}
        keywords = [word for word in words if len(word) > 1 and word not in stop_words]
        
        # è¿”å›å‡ºç°é¢‘æ¬¡è¾ƒé«˜çš„è¯
        from collections import Counter
        word_freq = Counter(keywords)
        return [word for word, freq in word_freq.most_common(10)]

    def _determine_policy_level(self, issuing_agency: Optional[str]) -> str:
        """ç¡®å®šæ”¿ç­–çº§åˆ«"""
        if not issuing_agency:
            return "æœªçŸ¥"
        
        agency_lower = issuing_agency.lower()
        if any(keyword in agency_lower for keyword in ['å›½åŠ¡é™¢', 'å‘æ”¹å§”', 'å·¥ä¿¡éƒ¨', 'ç§‘æŠ€éƒ¨']):
            return "å›½å®¶çº§"
        elif any(keyword in agency_lower for keyword in ['åŒ—äº¬å¸‚', 'å¸‚æ”¿åºœ', 'å¸‚å§”']):
            return "å¸‚çº§"
        elif any(keyword in agency_lower for keyword in ['åŒºæ”¿åºœ', 'åŒºå§”', 'è¡—é“']):
            return "åŒºçº§"
        else:
            return "å…¶ä»–"

    def extract_text_from_pdf(self, file_path: str) -> Tuple[str, List[Dict]]:
        """ä»PDFæå–æ–‡æœ¬å’Œè¡¨æ ¼"""
        try:
            # ä½¿ç”¨PyMuPDFæå–æ–‡æœ¬
            doc = fitz.open(file_path)
            text_content = ""
            tables = []
            
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                text_content += page.get_text()
            
            doc.close()
            
            # ä½¿ç”¨pdfplumberæå–è¡¨æ ¼
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_tables = page.extract_tables()
                    for table in page_tables:
                        if table:  # ç¡®ä¿è¡¨æ ¼ä¸ä¸ºç©º
                            tables.append({
                                'page': page_num + 1,
                                'content': table,
                                'text': self._table_to_text(table)
                            })
            
            return text_content, tables
            
        except Exception as e:
            logger.error(f"PDFè§£æå¤±è´¥ {file_path}: {e}")
            return "", []
    
    def extract_text_from_docx(self, file_path: str) -> str:
        """ä»DOCXæå–æ–‡æœ¬"""
        try:
            return docx2txt.process(file_path)
        except Exception as e:
            logger.error(f"DOCXè§£æå¤±è´¥ {file_path}: {e}")
            return ""
    
    def _table_to_text(self, table: List[List[str]]) -> str:
        """å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬"""
        if not table:
            return ""
        
        text_lines = []
        for row in table:
            if row and any(cell for cell in row if cell):  # è¿‡æ»¤ç©ºè¡Œ
                clean_row = [str(cell).strip() if cell else "" for cell in row]
                text_lines.append(" | ".join(clean_row))
        
        return "\n".join(text_lines)
    
    def extract_metadata(self, content: str, file_path: str) -> Dict[str, Any]:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        metadata = {
            'industries': [],
            'enterprise_scales': [],
            'policy_types': [],
            'requirements': [],
            'amounts': []
        }
        
        # æå–è¡Œä¸šä¿¡æ¯
        for industry, keywords in config.INDUSTRY_MAPPING.items():
            for keyword in keywords:
                if keyword in content:
                    metadata['industries'].append(industry)
                    break
        
        # æå–ä¼ä¸šè§„æ¨¡ä¿¡æ¯
        for scale, keywords in config.ENTERPRISE_SCALES.items():
            for keyword in keywords:
                if keyword in content:
                    metadata['enterprise_scales'].append(scale)
                    break
        
        # æå–æ”¿ç­–ç±»å‹
        for policy_type, keywords in config.POLICY_TYPES.items():
            for keyword in keywords:
                if keyword in content:
                    metadata['policy_types'].append(policy_type)
                    break
        
        # æå–é‡‘é¢ä¿¡æ¯
        amounts = self.money_pattern.findall(content)
        metadata['amounts'] = list(set(amounts))
        
        # æå–ç”³è¯·æ¡ä»¶
        requirements = []
        sentences = re.split(r'[ã€‚ï¼ï¼›\n]', content)
        for sentence in sentences:
            if self.requirement_pattern.search(sentence) and len(sentence) < 200:
                requirements.append(sentence.strip())
        
        metadata['requirements'] = requirements[:10]  # é™åˆ¶æ•°é‡
        
        return metadata
    
    def split_into_chunks(self, content: str, policy_id: str, tables: List[Dict] = None) -> List[PolicyChunk]:
        """å°†æ–‡æ¡£åˆ†å‰²æˆå—"""
        chunks = []
        
        # æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬
        paragraphs = re.split(r'\n\s*\n', content)
        
        current_section = "å¼€ç¯‡"
        chunk_id_counter = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para or len(para) < 10:
                continue
            
            # æ£€æµ‹æ˜¯å¦ä¸ºæ ‡é¢˜/ç« èŠ‚
            if self.title_pattern.match(para) or self.section_pattern.match(para):
                current_section = para[:50]  # å–å‰50å­—ç¬¦ä½œä¸ºç« èŠ‚å
            
            # å°†é•¿æ®µè½è¿›ä¸€æ­¥åˆ†å‰²
            if len(para) > config.MAX_CHUNK_SIZE:
                sub_chunks = self._split_long_paragraph(para)
                for sub_chunk in sub_chunks:
                    chunk_id = f"{policy_id}_chunk_{chunk_id_counter}"
                    chunks.append(PolicyChunk(
                        chunk_id=chunk_id,
                        policy_id=policy_id,
                        content=sub_chunk,
                        section=current_section,
                        keywords=self._extract_keywords(sub_chunk)
                    ))
                    chunk_id_counter += 1
            else:
                chunk_id = f"{policy_id}_chunk_{chunk_id_counter}"
                chunks.append(PolicyChunk(
                    chunk_id=chunk_id,
                    policy_id=policy_id,
                    content=para,
                    section=current_section,
                    keywords=self._extract_keywords(para)
                ))
                chunk_id_counter += 1
        
        # å¤„ç†è¡¨æ ¼
        if tables:
            for i, table in enumerate(tables):
                chunk_id = f"{policy_id}_table_{i}"
                chunks.append(PolicyChunk(
                    chunk_id=chunk_id,
                    policy_id=policy_id,
                    content=table['text'],
                    chunk_type="table",
                    page_num=table['page'],
                    keywords=self._extract_keywords(table['text'])
                ))
        
        return chunks
    
    def _split_long_paragraph(self, paragraph: str) -> List[str]:
        """åˆ†å‰²é•¿æ®µè½"""
        sentences = re.split(r'[ã€‚ï¼ï¼›]', paragraph)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk + sentence) < config.MAX_CHUNK_SIZE:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def generate_policy_id(self, file_path: str) -> str:
        """ç”Ÿæˆæ”¿ç­–ID"""
        file_name = Path(file_path).stem
        # ä½¿ç”¨æ–‡ä»¶åçš„hashä½œä¸ºIDçš„ä¸€éƒ¨åˆ†
        hash_obj = hashlib.md5(file_name.encode('utf-8'))
        return f"policy_{hash_obj.hexdigest()[:8]}"
    
    def process_document(self, file_path: str) -> PolicyDocument:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
        
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext == '.pdf':
            content, tables = self.extract_text_from_pdf(file_path)
        elif file_ext == '.docx':
            content = self.extract_text_from_docx(file_path)
            tables = []
        elif file_ext == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            tables = []
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        if not content.strip():
            raise ValueError(f"æ–‡æ¡£å†…å®¹ä¸ºç©º: {file_path}")
        
        # ç”Ÿæˆæ”¿ç­–ID
        policy_id = self.generate_policy_id(file_path)
        
        # æå–æ ‡é¢˜ï¼ˆå–æ–‡æ¡£å¼€å¤´çš„å‰100å­—ç¬¦ä½œä¸ºæ ‡é¢˜ï¼‰
        title = content[:100].replace('\n', ' ').strip()
        if not title:
            title = Path(file_path).stem
        
        # æå–å…ƒæ•°æ®
        metadata = self.extract_metadata(content, file_path)
        
        # æå–ç»“æ„åŒ–å­—æ®µ
        structured_fields = self.extractor.extract_structured_fields(content)
        
        # åˆ†å‰²æ–‡æ¡£
        chunks = self.split_into_chunks(content, policy_id, tables)
        
        # åˆ›å»ºæ”¿ç­–æ–‡æ¡£å¯¹è±¡
        policy_doc = PolicyDocument(
            policy_id=policy_id,
            title=title,
            content=content,
            industry=metadata['industries'],
            enterprise_scale=metadata['enterprise_scales'],
            policy_type=metadata['policy_types'][0] if metadata['policy_types'] else None,
            file_path=file_path,
            chunks=chunks,
            # ğŸ†• æ·»åŠ ç”¨äºæ•°æ®åº“å…³è”çš„å­—æ®µ
            original_filename=Path(file_path).name,  # åŸå§‹æ–‡ä»¶åï¼ˆåŒ…å«æ‰©å±•åï¼‰
            document_number=structured_fields.get('document_number'),
            issuing_agency=structured_fields.get('issuing_agency')
        )
        
        logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {file_path}, ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
        return policy_doc 